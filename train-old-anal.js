import * as tf from '@tensorflow/tfjs-node';
import fs from 'fs/promises';
import path from 'path';
import cliProgress from 'cli-progress';

// Ð Ð°Ð·Ð±Ð¾Ñ€ Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² ÐºÐ¾Ð¼Ð°Ð½Ð´Ð½Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐ¸
const args = process.argv.slice(2).reduce((acc, cur, i, arr) => {
  if (cur.startsWith('--')) {
    acc[cur.slice(2)] = arr[i + 1];
  }
  return acc;
}, {});

const MODEL_NAME  = args.model ?? 'default-model';
const DATA_DIR    = './converted-images';
const METRIC_DIR  = './metrics-json';       // ÐŸÐ°Ð¿ÐºÐ° Ñ json-Ñ„Ð°Ð¹Ð»Ð°Ð¼Ð¸ Ð¼ÐµÑ‚Ñ€Ð¸Ðº
const MODEL_DIR   = './cnn-models';
const EPOCHS      = 10;
const BATCH_SIZE  = 64;
const IMAGE_HEIGHT = 125;
const IMAGE_WIDTH  = 125;
const NUM_OUTPUTS  = 8;                  // 4 Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð² + 4 Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ ÑÑ‚Ñ€Ð¾Ðº

function createModel() {
  const model = tf.sequential();

  // Ð¡Ð²Ñ‘Ñ€Ñ‚Ð¾Ñ‡Ð½Ñ‹Ð¹ Ð±Ð»Ð¾Ðº 1
  model.add(tf.layers.conv2d({
    inputShape: [IMAGE_HEIGHT, IMAGE_WIDTH, 1],
    filters: 32,
    kernelSize: 3,
    padding: 'same',
    useBias: false
  }));
  model.add(tf.layers.batchNormalization());
  model.add(tf.layers.leakyReLU({ alpha: 0.1 }));
  model.add(tf.layers.maxPooling2d({ poolSize: [2, 2] }));
  model.add(tf.layers.dropout({ rate: 0.10 }));

  // Ð¡Ð²Ñ‘Ñ€Ñ‚Ð¾Ñ‡Ð½Ñ‹Ð¹ Ð±Ð»Ð¾Ðº 2
  model.add(tf.layers.conv2d({ filters: 64, kernelSize: 3, padding: 'same', useBias: false }));
  model.add(tf.layers.batchNormalization());
  model.add(tf.layers.leakyReLU({ alpha: 0.1 }));
  model.add(tf.layers.maxPooling2d({ poolSize: [2, 2] }));
  model.add(tf.layers.dropout({ rate: 0.10 }));

  // Ð¡Ð²Ñ‘Ñ€Ñ‚Ð¾Ñ‡Ð½Ñ‹Ð¹ Ð±Ð»Ð¾Ðº 3
  model.add(tf.layers.conv2d({ filters: 128, kernelSize: 3, padding: 'same', useBias: false }));
  model.add(tf.layers.batchNormalization());
  model.add(tf.layers.leakyReLU({ alpha: 0.1 }));
  model.add(tf.layers.maxPooling2d({ poolSize: [2, 2] }));
  model.add(tf.layers.dropout({ rate: 0.5 }));

  // Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð¾Ðµ ÑƒÑÑ€ÐµÐ´Ð½ÐµÐ½Ð¸Ðµ Ð²Ð¼ÐµÑÑ‚Ð¾ Flatten
  model.add(tf.layers.globalAveragePooling2d({ dataFormat: 'channelsLast' }));

  // ÐŸÐ¾Ð»Ð½Ð¾ÑÐ²ÑÐ·Ð½Ð°Ñ Ñ‡Ð°ÑÑ‚ÑŒ
  model.add(tf.layers.dense({ units: 128 }));
  model.add(tf.layers.leakyReLU({ alpha: 0.1 }));
  model.add(tf.layers.dropout({ rate: 0.30 }));

  // Ð’Ñ‹Ñ…Ð¾Ð´Ð½Ð¾Ð¹ ÑÐ»Ð¾Ð¹ (Ð»Ð¸Ð½ÐµÐ¹Ð½Ð°Ñ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ñ)
  model.add(tf.layers.dense({ units: NUM_OUTPUTS }));

  model.compile({
    optimizer: tf.train.adam(0.001),
    loss: 'meanSquaredError',
    metrics: ['mse']
  });
  return model;
}

async function train() {
  const files = (await fs.readdir(DATA_DIR)).filter(f => f.endsWith('.jpg'));
  const numSamples   = files.length;
  const totalBatches = Math.ceil(numSamples / BATCH_SIZE);

  async function* bufferGenerator() {
    for (const f of files) {
      const imgBuffer = await fs.readFile(path.join(DATA_DIR, f));
      // Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ð¹ JSON-Ñ„Ð°Ð¹Ð» Ñ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ°Ð¼Ð¸
      const baseName = path.basename(f, '.jpg');
      const jsonBuffer = await fs.readFile(path.join(METRIC_DIR, `${baseName}.json`), 'utf-8');
      const meta = JSON.parse(jsonBuffer);
      // Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð² Ð¾Ð´Ð¸Ð½ Ð¼Ð°ÑÑÐ¸Ð²: ÑÐ½Ð°Ñ‡Ð°Ð»Ð° Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð², Ð¿Ð¾Ñ‚Ð¾Ð¼ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ ÑÑ‚Ñ€Ð¾Ðº
      const labelArray = [
        ...meta.averageCharMetrics,
        ...meta.averageLineMetrics
      ];
      yield { buffer: imgBuffer, label: labelArray };
    }
  }

  const ds = tf.data
    .generator(bufferGenerator)
    .shuffle(numSamples)
    .map(({ buffer, label }) => tf.tidy(() => {
      const img = tf.node
        .decodeImage(buffer, 1)
        .resizeNearestNeighbor([IMAGE_HEIGHT, IMAGE_WIDTH])
        .toFloat()
        .div(255.0);
      const y = tf.tensor1d(label);  // Ð¼ÐµÑ‚ÐºÐ¸ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð¸
      return { xs: img, ys: y };
    }))
    .batch(BATCH_SIZE)
    .prefetch(1);

  const model = createModel();

  const epochBar = new cliProgress.SingleBar({ format: 'Ð­Ð¿Ð¾Ñ…Ð° {value}/{total}' }, cliProgress.Presets.shades_classic);
  epochBar.start(EPOCHS, 0);

  let batchBar;
  await model.fitDataset(ds, {
    epochs: EPOCHS,
    callbacks: {
      onEpochBegin: async (epoch) => {
        batchBar = new cliProgress.SingleBar({ format: `Ð­Ð¿Ð¾Ñ…Ð° ${epoch+1} [{bar}] Ð‘Ð°Ñ‚Ñ‡ {value}/${totalBatches}` }, cliProgress.Presets.shades_classic);
        batchBar.start(totalBatches, 0);
      },
      onBatchEnd: async (batch) => {
        batchBar.update(batch+1);
      },
      onEpochEnd: async (epoch, logs) => {
        batchBar.stop();
        epochBar.increment();
      
        console.log(` [Ð­Ð¿Ð¾Ñ…Ð° ${epoch+1} â€” loss=${logs.loss.toFixed(4)}, mse=${logs.mse.toFixed(4)}]`);
      
        // Ð’Ñ‹Ð²Ð¾Ð´ Ð¼ÐµÑ‚Ñ€Ð¸Ðº Ð¿Ð¾ ÐºÐ°Ð¶Ð´Ð¾Ð¼Ñƒ Ð²Ñ‹Ñ…Ð¾Ð´Ñƒ
        const preds = model.predict(tf.randomUniform([1, IMAGE_HEIGHT, IMAGE_WIDTH, 1]));
        if (preds.shape[1] === NUM_OUTPUTS) {
          console.log('ðŸ§  ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð½Ð¾Ð³Ð¾ Ð²Ñ…Ð¾Ð´Ð°:', Array.from(await preds.data()).map(v => v.toFixed(3)));
        }
      }      
    }
  });

  epochBar.stop();
  console.log('\nâœ… Training complete');

  await fs.mkdir(MODEL_DIR, { recursive: true });
  await model.save(`file://${path.join(MODEL_DIR, MODEL_NAME)}`);
  console.log(`âœ… ÐœÐ¾Ð´ÐµÐ»ÑŒ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð° Ð² ${path.join(MODEL_DIR, MODEL_NAME)}`);
}

train().catch(err => {
  console.error(err);
  process.exit(1);
});
